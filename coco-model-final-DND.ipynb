{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(42)\n",
    "rn.seed(12345)\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "from keras import backend as K\n",
    "tf.set_random_seed(1234)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.0'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from pandas import DataFrame\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from pickle import load\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "import h5py\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.pooling import GlobalMaxPooling2D\n",
    "from keras.layers.pooling import GlobalAveragePooling2D\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "import pandas as pd\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from pickle import dump\n",
    "from gensim.models import Word2Vec\n",
    "from os import listdir\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from matplotlib import pyplot\n",
    "import string\n",
    "import json\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list=os.listdir(\"coco/full/train2017\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_descriptions(filename='file',image_list=[]):\n",
    "    with open(filename) as json_data:\n",
    "        f=json.load(json_data)\n",
    "    \n",
    "    description=dict()\n",
    "    for i in range(len(f[\"annotations\"])):\n",
    "        if \"caption\" in f[\"annotations\"][i].keys():\n",
    "            image_id=f[\"annotations\"][i][\"image_id\"]\n",
    "            image_id=format(image_id, '012d')\n",
    "            if image_id+\".jpg\" in image_list:\n",
    "                description[image_id]=f[\"annotations\"][i][\"caption\"]\n",
    "            \n",
    "    return(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#descriptions = load_descriptions(\"coco/annotations/captions_train2017.json\",train_list)\n",
    "#print('Loaded: %d ' % len(descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_descriptions(descriptions):\n",
    "    tree_tokenize=TreebankWordTokenizer()\n",
    "    for key, desc in descriptions.items():    \n",
    "        desc=desc.lower()\n",
    "        desc=tree_tokenize.tokenize(desc)\n",
    "        desc=' '.join(desc)\n",
    "        desc=desc.replace(\"n't\",\"not\")\n",
    "        desc=desc.replace(\"'s\",\"\")\n",
    "        for c in string.punctuation:\n",
    "            desc=desc.replace(c,\"\")\n",
    "        desc=desc.split()\n",
    "        desc = [word for word in desc if len(word)>1]\n",
    "        descriptions[key] =  ' '.join(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_descriptions(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_tokens = ' '.join(descriptions.values()).split()\n",
    "# vocabulary = set(all_tokens)\n",
    "# print('Number of words in vocabulary: %d ' % len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_doc(descriptions, filename):\n",
    "    lines = list()\n",
    "    for key, desc in descriptions.items():\n",
    "        lines.append(key + ' ' + desc)\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_doc(descriptions,\"coco/full/description.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(input_directory=\"file\",output_directory=\"file\"):\n",
    "    # load the model\n",
    "    import h5py\n",
    "    f = h5py.File(output_directory)\n",
    "    in_layer = Input(shape=(224, 224, 3))\n",
    "    model = InceptionV3(include_top=False, input_tensor=in_layer)\n",
    "    print(model.summary())\n",
    "    # extract features from each photo\n",
    "    features = dict()\n",
    "    i=0\n",
    "    for name in listdir(input_directory):\n",
    "        # load an image from file\n",
    "        filename = input_directory + '/' + name\n",
    "        image = load_img(filename, target_size=(224, 224))\n",
    "        # convert the image pixels to a numpy array\n",
    "        image = img_to_array(image)\n",
    "        # reshape data for the model\n",
    "        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "        # prepare the image for the Inception model\n",
    "        image = preprocess_input(image)\n",
    "        # get features\n",
    "        feature = model.predict(image, verbose=0)\n",
    "        # get image id\n",
    "        image_id = name.split('.')[0]\n",
    "        # store feature\n",
    "        f.create_dataset(image_id, data=np.array(feature, dtype=np.float32))\n",
    "        print(i)\n",
    "        i+=1\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract_features(input_directory=\"coco/full/train2017\",output_directory=\"coco/full/features.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_descriptions(filename):\n",
    "    # load document\n",
    "    doc = load_doc(filename)\n",
    "    descriptions = dict()\n",
    "    for line in doc.split('\\n'):\n",
    "        # split line by white space\n",
    "        tokens = line.split()\n",
    "        # split id from description\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        # skip images not in the set\n",
    "        descriptions[image_id] = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "    return descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_descriptions=load_clean_descriptions(\"coco/full/description.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=train_descriptions.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(descriptions):\n",
    "    lines = list(descriptions.values())\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = create_tokenizer(train_descriptions)\n",
    "#vocab_size = len(tokenizer.word_index) + 1\n",
    "#print('Vocabulary Size: %d' % vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_to_index=pd.Series(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_to_index.to_csv(\"coco/full/tokens.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index=pd.read_csv(\"coco/full/tokens.csv\",header=None, index_col=0, squeeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_to_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sequences(desc,tokens):    \n",
    "    text_to_sequences=list()\n",
    "    for i in desc.split():\n",
    "        if i==\"nan\":\n",
    "            continue\n",
    "        text_to_sequences.append(tokens[i])\n",
    "    return text_to_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max(len(s.split()) for s in list(train_descriptions.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sequences of images, input sequences and output words for an image\n",
    "def create_sequences(tokenizer, desc, image, max_length):\n",
    "    Ximages, XSeq, y = list(), list(),list()\n",
    "    vocab_size = len(tokenizer) + 1\n",
    "    # integer encode the description\n",
    "    seq = text_to_sequences(desc,tokenizer)\n",
    "    # split one sequence into multiple X,y pairs\n",
    "    for i in range(1, len(seq)):\n",
    "        # select\n",
    "        in_seq, out_seq = seq[:i], seq[i]\n",
    "        # pad input sequence\n",
    "        in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "        # encode output sequence\n",
    "        out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "        # store\n",
    "        Ximages.append(image)\n",
    "        XSeq.append(in_seq)\n",
    "        y.append(out_seq)\n",
    "    # Ximages, XSeq, y = array(Ximages), array(XSeq), array(y)\n",
    "    return [Ximages, XSeq, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(descriptions, features_directory, tokenizer, max_length, n_step):\n",
    "    # loop until we finish training\n",
    "    f = h5py.File(features_directory, 'r')\n",
    "    while 1:\n",
    "        # loop over photo identifiers in the dataset\n",
    "        keys = list(descriptions.keys())\n",
    "        for i in range(0, len(keys), n_step):\n",
    "            Ximages, XSeq, y = list(), list(),list()\n",
    "            for j in range(i, min(len(keys), i+n_step)):\n",
    "                image_id = keys[j]\n",
    "                # retrieve photo feature input\n",
    "                image = f[image_id].value[0]\n",
    "                # retrieve text input\n",
    "                desc = descriptions[image_id]\n",
    "                # generate input-output pairs\n",
    "                in_img, in_seq, out_word = create_sequences(tokenizer, desc, image, max_length)\n",
    "                for k in range(len(in_img)):\n",
    "                    Ximages.append(in_img[k])\n",
    "                    XSeq.append(in_seq[k])\n",
    "                    y.append(out_word[k])\n",
    "            # yield this batch of samples to the model\n",
    "            yield [[array(Ximages), array(XSeq)], array(y)]\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_for_id(integer, tokenizer):\n",
    "    if integer in tokenizer.values:\n",
    "        return word_to_index[word_to_index==integer].index[0]\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_desc_beam(model, tokenizer, photo, max_length,k=10):\n",
    "    # seed the generation process\n",
    "    in_text = \"\"\n",
    "    sequences = [[list([1]), 1.0]]\n",
    "    # iterate over the whole length of the sequence\n",
    "    for i in range(max_length-1):\n",
    "        # integer encode input sequence\n",
    "        all_candidates = list()\n",
    "        for i in range(len(sequences)):\n",
    "            seq, score = sequences[i]\n",
    "            sequence = pad_sequences([seq], maxlen=max_length)\n",
    "            predict=model.predict([photo,sequence],verbose=0)\n",
    "            for j in range(len(predict[0])):\n",
    "                candidate = [seq + [j], score * -np.log(predict[0][j]+1e-323)]\n",
    "                all_candidates.append(candidate)\n",
    "        ordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
    "        sequences = ordered[:k]\n",
    "        word=word_for_id(sequences[0][0][-1], tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    for i in sequences[0][0]:\n",
    "        word=word_for_id(i, tokenizer)\n",
    "        in_text=in_text+\" \"+word\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lines = [s.split() for s in train_descriptions.values()]\n",
    "# model = Word2Vec(lines, size=500, window=5, workers=8, min_count=1)\n",
    "# # summarize vocabulary size in model\n",
    "# words = list(model.wv.vocab)\n",
    "# print('Vocabulary size: %d' % len(words))\n",
    " \n",
    "# # save model in ASCII (word2vec) format\n",
    "# filename = 'coco/full/custom_embedding.txt'\n",
    "# model.wv.save_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the whole embedding into memory\n",
    "# embedding = dict()\n",
    "# file = open('coco/full/custom_embedding.txt')\n",
    "# for line in file:\n",
    "# \tvalues = line.split()\n",
    "# \tword = values[0]\n",
    "# \tcoefs = asarray(values[1:], dtype='float32')\n",
    "# \tembedding[word] = coefs\n",
    "# file.close()\n",
    "# print('Embedding Size: %d' % len(embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # summarize vocabulary\n",
    "# all_tokens = ' '.join(train_descriptions.values()).split()\n",
    "# vocabulary = set(all_tokens)\n",
    "# print('Vocabulary Size: %d' % len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cust_embedding = dict()\n",
    "# for word in vocabulary:\n",
    "# \t# check if word in embedding\n",
    "# \tif word not in embedding:\n",
    "# \t\tcontinue\n",
    "# \tcust_embedding[word] = embedding[word]\n",
    "# print('Custom Embedding %d' % len(cust_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump(cust_embedding, open('coco/full/word2vec_embedding.pkl', 'wb'))\n",
    "# print('Saved Embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding(tokenizer, vocab_size, max_length):\n",
    "    # load the tokenizer\n",
    "    embedding = load(open('coco/full/word2vec_embedding.pkl', 'rb'))\n",
    "    dimensions = 500\n",
    "    trainable = False\n",
    "    # create a weight matrix for words in training docs\n",
    "    weights = zeros((vocab_size, dimensions))\n",
    "    # walk words in order of tokenizer vocab to ensure vectors are in the right index\n",
    "    for i in tokenizer.index:\n",
    "        if i not in embedding:\n",
    "            continue\n",
    "        weights[tokenizer[i]] = embedding[i]\n",
    "    layer = Embedding(vocab_size, dimensions, weights=[weights], input_length=max_length, trainable=trainable, mask_zero=True)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(tokenizer, vocab_size, max_length):\n",
    "    # feature extractor (encoder)\n",
    "    inputs1 = Input(shape=(5, 5, 2048))\n",
    "    fe1 = GlobalMaxPooling2D()(inputs1)\n",
    "    fe2 = Dense(128, activation='relu')(fe1)\n",
    "    fe3 = RepeatVector(max_length)(fe2)\n",
    "    # embedding\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    emb2 = load_embedding(tokenizer, vocab_size, max_length)(inputs2)\n",
    "    emb3 = LSTM(256, return_sequences=True)(emb2)\n",
    "    emb4 = TimeDistributed(Dense(128, activation='relu'))(emb3)\n",
    "    # merge inputs\n",
    "    merged = concatenate([fe3, emb4])\n",
    "    # language model (decoder)\n",
    "    lm2 = LSTM(256)(merged)\n",
    "    lm3 = Dense(256, activation='relu')(lm2)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(lm3)\n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = 1\n",
    "n_photos_per_update = 128\n",
    "n_batches_per_epoch = len(train_descriptions)/n_photos_per_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = define_model(word_to_index,vocab_size, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "925/924 [==============================] - 3615s 4s/step - loss: 4.8208 - acc: 0.2182\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2d670516240>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(data_generator(train_descriptions,features_directory=\"coco/full/features.h5\", tokenizer=word_to_index,max_length= max_length,n_step= n_photos_per_update), steps_per_epoch=n_batches_per_epoch, epochs=1, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/6\n",
      "925/924 [==============================] - 3673s 4s/step - loss: 3.5903 - acc: 0.3296\n",
      "Epoch 3/6\n",
      "925/924 [==============================] - 3601s 4s/step - loss: 3.3272 - acc: 0.3519\n",
      "Epoch 4/6\n",
      "925/924 [==============================] - 3589s 4s/step - loss: 3.1841 - acc: 0.3640\n",
      "Epoch 5/6\n",
      "925/924 [==============================] - 3644s 4s/step - loss: 3.0846 - acc: 0.3710\n",
      "Epoch 6/6\n",
      "925/924 [==============================] - 3713s 4s/step - loss: 3.0054 - acc: 0.3771\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2d69a923b38>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(data_generator(train_descriptions,features_directory=\"coco/full/features.h5\", tokenizer=word_to_index,max_length= max_length,n_step= n_photos_per_update), steps_per_epoch=n_batches_per_epoch, epochs=6,initial_epoch=1, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.models.save_model(model,\"coco/full/checkpoints/epoch9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/46\n",
      "925/924 [==============================] - 3938s 4s/step - loss: 2.3569 - acc: 0.4535\n",
      "Epoch 39/46\n",
      "925/924 [==============================] - 3750s 4s/step - loss: 2.3508 - acc: 0.4546\n",
      "Epoch 40/46\n",
      "925/924 [==============================] - 3680s 4s/step - loss: 2.3451 - acc: 0.4553\n",
      "Epoch 41/46\n",
      "925/924 [==============================] - 3717s 4s/step - loss: 2.3412 - acc: 0.4558\n",
      "Epoch 42/46\n",
      "925/924 [==============================] - 3753s 4s/step - loss: 2.3414 - acc: 0.4555\n",
      "Epoch 43/46\n",
      "925/924 [==============================] - 3816s 4s/step - loss: 2.3366 - acc: 0.4566\n",
      "Epoch 44/46\n",
      "925/924 [==============================] - 3834s 4s/step - loss: 2.3260 - acc: 0.4581\n",
      "Epoch 45/46\n",
      "925/924 [==============================] - 3870s 4s/step - loss: 2.3204 - acc: 0.4588\n",
      "Epoch 46/46\n",
      "925/924 [==============================] - 3894s 4s/step - loss: 2.3157 - acc: 0.4597\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x10f05677978>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(data_generator(train_descriptions,features_directory=\"coco/full/features.h5\", tokenizer=word_to_index,max_length= max_length,n_step= n_photos_per_update), steps_per_epoch=n_batches_per_epoch, epochs=46,initial_epoch=37, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_value(model.optimizer.lr,\".0001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/86\n",
      "925/924 [==============================] - 4107s 4s/step - loss: 2.0369 - acc: 0.5101\n",
      "Epoch 80/86\n",
      "925/924 [==============================] - 3931s 4s/step - loss: 2.0204 - acc: 0.5144\n",
      "Epoch 81/86\n",
      "925/924 [==============================] - 3835s 4s/step - loss: 2.0099 - acc: 0.5170\n",
      "Epoch 82/86\n",
      "925/924 [==============================] - 3806s 4s/step - loss: 2.0014 - acc: 0.5190\n",
      "Epoch 83/86\n",
      "925/924 [==============================] - 3815s 4s/step - loss: 1.9940 - acc: 0.5207\n",
      "Epoch 84/86\n",
      "925/924 [==============================] - 3822s 4s/step - loss: 1.9872 - acc: 0.5222\n",
      "Epoch 85/86\n",
      "925/924 [==============================] - 3852s 4s/step - loss: 1.9809 - acc: 0.5237\n",
      "Epoch 86/86\n",
      "925/924 [==============================] - 3906s 4s/step - loss: 1.9751 - acc: 0.5251\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2bbebe33898>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(data_generator(train_descriptions,features_directory=\"coco/full/features.h5\", tokenizer=word_to_index,max_length= max_length,n_step= n_photos_per_update), steps_per_epoch=n_batches_per_epoch, epochs=86,initial_epoch=78, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.models.save_model(model,\"coco/full/checkpoints/epoch122\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/98\n",
      "925/924 [==============================] - 3795s 4s/step - loss: 1.9469 - acc: 0.5312\n",
      "Epoch 93/98\n",
      "925/924 [==============================] - 3718s 4s/step - loss: 1.9424 - acc: 0.5323\n",
      "Epoch 94/98\n",
      "925/924 [==============================] - 3693s 4s/step - loss: 1.9381 - acc: 0.5333\n",
      "Epoch 95/98\n",
      "925/924 [==============================] - 3706s 4s/step - loss: 1.9340 - acc: 0.5342\n",
      "Epoch 96/98\n",
      "925/924 [==============================] - 3739s 4s/step - loss: 1.9301 - acc: 0.5351\n",
      "Epoch 97/98\n",
      "925/924 [==============================] - 3778s 4s/step - loss: 1.9263 - acc: 0.5360\n",
      "Epoch 98/98\n",
      "925/924 [==============================] - 3810s 4s/step - loss: 1.9225 - acc: 0.5368\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21c6b07df98>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(data_generator(train_descriptions,features_directory=\"coco/full/features.h5\", tokenizer=word_to_index,max_length= max_length,n_step= n_photos_per_update), steps_per_epoch=n_batches_per_epoch, epochs=98,initial_epoch=91, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151/158\n",
      "925/924 [==============================] - 3730s 4s/step - loss: 1.8064 - acc: 0.5617\n",
      "Epoch 152/158\n",
      "925/924 [==============================] - 3789s 4s/step - loss: 1.8010 - acc: 0.5634\n",
      "Epoch 153/158\n",
      "925/924 [==============================] - 3880s 4s/step - loss: 1.7977 - acc: 0.5642\n",
      "Epoch 154/158\n",
      "925/924 [==============================] - 3807s 4s/step - loss: 1.7953 - acc: 0.5648\n",
      "Epoch 155/158\n",
      "925/924 [==============================] - 3781s 4s/step - loss: 1.7931 - acc: 0.5654\n",
      "Epoch 156/158\n",
      "925/924 [==============================] - 3757s 4s/step - loss: 1.7911 - acc: 0.5660\n",
      "Epoch 157/158\n",
      "925/924 [==============================] - 3790s 4s/step - loss: 1.7890 - acc: 0.5665\n",
      "Epoch 158/158\n",
      "925/924 [==============================] - 4033s 4s/step - loss: 1.7871 - acc: 0.5669\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x228a3922208>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(data_generator(train_descriptions,features_directory=\"coco/full/features.h5\", tokenizer=word_to_index,max_length= max_length,n_step= n_photos_per_update), steps_per_epoch=n_batches_per_epoch, epochs=158,initial_epoch=150, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 178/189\n",
      "925/924 [==============================] - 3753s 4s/step - loss: 1.7583 - acc: 0.5734\n",
      "Epoch 179/189\n",
      "925/924 [==============================] - 3759s 4s/step - loss: 1.7558 - acc: 0.5741\n",
      "Epoch 180/189\n",
      "925/924 [==============================] - 3815s 4s/step - loss: 1.7541 - acc: 0.5745\n",
      "Epoch 181/189\n",
      "925/924 [==============================] - 35545s 38s/step - loss: 1.7524 - acc: 0.5750\n",
      "Epoch 182/189\n",
      "925/924 [==============================] - 3880s 4s/step - loss: 1.7508 - acc: 0.5754\n",
      "Epoch 183/189\n",
      "925/924 [==============================] - 3815s 4s/step - loss: 1.7493 - acc: 0.5758\n",
      "Epoch 184/189\n",
      "925/924 [==============================] - 3768s 4s/step - loss: 1.7478 - acc: 0.5761\n",
      "Epoch 185/189\n",
      "925/924 [==============================] - 4282s 5s/step - loss: 1.7464 - acc: 0.5766\n",
      "Epoch 186/189\n",
      "925/924 [==============================] - 3816s 4s/step - loss: 1.7450 - acc: 0.5768\n",
      "Epoch 187/189\n",
      "925/924 [==============================] - 3857s 4s/step - loss: 1.7437 - acc: 0.5771\n",
      "Epoch 188/189\n",
      "925/924 [==============================] - 3815s 4s/step - loss: 1.7423 - acc: 0.5774\n",
      "Epoch 189/189\n",
      "925/924 [==============================] - 3857s 4s/step - loss: 1.7410 - acc: 0.5777\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c97ccf6e80>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(data_generator(dict(sorted(train_descriptions.items())),features_directory=\"coco/full/features.h5\", tokenizer=word_to_index,max_length= max_length,n_step= n_photos_per_update), steps_per_epoch=n_batches_per_epoch, epochs=189,initial_epoch=177, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=keras.models.load_model(\"coco/full/checkpoints/epoch240\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_value(model.optimizer.lr,\".00001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 190/195\n",
      "925/924 [==============================] - 4154s 4s/step - loss: 1.7261 - acc: 0.5810\n",
      "Epoch 191/195\n",
      "925/924 [==============================] - 3996s 4s/step - loss: 1.7211 - acc: 0.5824\n",
      "Epoch 192/195\n",
      "925/924 [==============================] - 3986s 4s/step - loss: 1.7194 - acc: 0.5829\n",
      "Epoch 193/195\n",
      "925/924 [==============================] - 3827s 4s/step - loss: 1.7184 - acc: 0.5831\n",
      "Epoch 194/195\n",
      "925/924 [==============================] - 3774s 4s/step - loss: 1.7176 - acc: 0.5834\n",
      "Epoch 195/195\n",
      "925/924 [==============================] - 3742s 4s/step - loss: 1.7169 - acc: 0.5836\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23cf75d0940>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(data_generator(dict(sorted(train_descriptions.items())),features_directory=\"coco/full/features.h5\", tokenizer=word_to_index,max_length= max_length,n_step= n_photos_per_update), steps_per_epoch=n_batches_per_epoch, epochs=195,initial_epoch=189, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=keras.models.load_model(\"coco/full/checkpoints/epoch224\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 225/232\n",
      "925/924 [==============================] - 4086s 4s/step - loss: 1.7079 - acc: 0.5858\n",
      "Epoch 226/232\n",
      "925/924 [==============================] - 3973s 4s/step - loss: 1.7076 - acc: 0.5859\n",
      "Epoch 227/232\n",
      "925/924 [==============================] - 3960s 4s/step - loss: 1.7073 - acc: 0.5859\n",
      "Epoch 228/232\n",
      "925/924 [==============================] - 3914s 4s/step - loss: 1.7071 - acc: 0.5860\n",
      "Epoch 229/232\n",
      "925/924 [==============================] - 3848s 4s/step - loss: 1.7069 - acc: 0.5860\n",
      "Epoch 230/232\n",
      "925/924 [==============================] - 3811s 4s/step - loss: 1.7067 - acc: 0.5861\n",
      "Epoch 231/232\n",
      "925/924 [==============================] - 3811s 4s/step - loss: 1.7064 - acc: 0.5861\n",
      "Epoch 232/232\n",
      "925/924 [==============================] - 3833s 4s/step - loss: 1.7062 - acc: 0.5862\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a4748d1a58>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(data_generator(dict(sorted(train_descriptions.items())),features_directory=\"coco/full/features.h5\", tokenizer=word_to_index,max_length= max_length,n_step= n_photos_per_update), steps_per_epoch=n_batches_per_epoch, epochs=232,initial_epoch=224,verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 233/240\n",
      "925/924 [==============================] - 5228s 6s/step - loss: 1.7060 - acc: 0.5862\n",
      "Epoch 234/240\n",
      "925/924 [==============================] - 3871s 4s/step - loss: 1.7058 - acc: 0.5863\n",
      "Epoch 235/240\n",
      "925/924 [==============================] - 3853s 4s/step - loss: 1.7056 - acc: 0.5863\n",
      "Epoch 236/240\n",
      "925/924 [==============================] - 3905s 4s/step - loss: 1.7054 - acc: 0.5864\n",
      "Epoch 237/240\n",
      "925/924 [==============================] - 3896s 4s/step - loss: 1.7052 - acc: 0.5864\n",
      "Epoch 238/240\n",
      "925/924 [==============================] - 3929s 4s/step - loss: 1.7050 - acc: 0.5864\n",
      "Epoch 239/240\n",
      "925/924 [==============================] - 3912s 4s/step - loss: 1.7048 - acc: 0.5865\n",
      "Epoch 240/240\n",
      "925/924 [==============================] - 3874s 4s/step - loss: 1.7046 - acc: 0.5865\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a47d0f5828>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(data_generator(dict(sorted(train_descriptions.items())),features_directory=\"coco/full/features.h5\", tokenizer=word_to_index,max_length= max_length,n_step= n_photos_per_update), steps_per_epoch=n_batches_per_epoch, epochs=240,initial_epoch=232,verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=keras.models.load_model(\"coco/full/checkpoints/epoch240\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 241/248\n",
      "925/924 [==============================] - 2780s 3s/step - loss: 1.7050 - acc: 0.5865\n",
      "Epoch 242/248\n",
      "925/924 [==============================] - 2664s 3s/step - loss: 1.7047 - acc: 0.5866\n",
      "Epoch 243/248\n",
      "925/924 [==============================] - 2707s 3s/step - loss: 1.7044 - acc: 0.5866\n",
      "Epoch 244/248\n",
      "925/924 [==============================] - 2826s 3s/step - loss: 1.7042 - acc: 0.5867\n",
      "Epoch 245/248\n",
      "925/924 [==============================] - 2801s 3s/step - loss: 1.7040 - acc: 0.5867\n",
      "Epoch 246/248\n",
      "925/924 [==============================] - 2793s 3s/step - loss: 1.7038 - acc: 0.5867\n",
      "Epoch 247/248\n",
      "925/924 [==============================] - 2817s 3s/step - loss: 1.7036 - acc: 0.5868\n",
      "Epoch 248/248\n",
      "925/924 [==============================] - 2986s 3s/step - loss: 1.7034 - acc: 0.5868\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17539115f98>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(data_generator(dict(sorted(train_descriptions.items())),features_directory=\"coco/full/features.h5\", tokenizer=word_to_index,max_length= max_length,n_step= n_photos_per_update), steps_per_epoch=n_batches_per_epoch, epochs=248,initial_epoch=240,verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.models.save_model(model,\"coco/full/checkpoints/epoch248\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=keras.models.load_model(\"coco/full/checkpoints/epoch248\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_list=os.listdir(\"coco/full/val2017\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#descriptions = load_descriptions(\"coco/annotations/captions_val2017.json\",image_list=val_list)\n",
    "#print('Loaded: %d ' % len(descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_descriptions(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_doc(descriptions,\"coco/full/val_description.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract_features(input_directory=\"coco/full/val2017\",output_directory=\"coco/full/val_features.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_descriptions=load_clean_descriptions(\"coco/full/val_description.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, descriptions, features_directory, tokenizer, max_length):\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    f = h5py.File(features_directory, 'r')\n",
    "    actual, predicted = list(), list()\n",
    "    i=0\n",
    "    for key, desc in descriptions.items():\n",
    "        # generate description\n",
    "        yhat = generate_desc_beam(model, tokenizer, f[key].value, max_length,k=3)\n",
    "        # store actual and predicted\n",
    "        actual.append([desc.split()])\n",
    "        predicted.append(yhat.split())\n",
    "        print(i)\n",
    "        i=i+1\n",
    "    # calculate BLEU score\n",
    "    bleu = corpus_bleu(actual, predicted,smoothing_function=smoothie)\n",
    "    f.close()\n",
    "    return bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model1(model, descriptions, features_directory, tokenizer, max_length,dataset=\"train2017\"):\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    f = h5py.File(features_directory, 'r')\n",
    "    actual, predicted = list(), list()\n",
    "    for key, desc in descriptions.items():\n",
    "        # generate description\n",
    "        yhat = generate_desc_beam(model, tokenizer, f[key].value, max_length,k=10)\n",
    "        # store actual and predicted\n",
    "        actual.append([desc.split()])\n",
    "        print(key)\n",
    "        predicted.append(yhat.split())\n",
    "        print('Actual:    %s' % desc)\n",
    "        print('Predicted: %s' % yhat)\n",
    "        if len(actual) >= 10:\n",
    "            break\n",
    "    # calculate BLEU score\n",
    "    bleu = corpus_bleu(actual, predicted,smoothing_function=smoothie)\n",
    "    f.close()\n",
    "    return bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000000011156\n",
      "Actual:    startseq purse laptop ipod cell phone coins earphones and pencil laying on bed endseq\n",
      "Predicted:  startseq cellphone and pens holder next to cell phone next to it endseq\n",
      "000000370479\n",
      "Actual:    startseq kitchen with chef pointing at another person near the stove endseq\n",
      "Predicted:  startseq group of people gathered around kitchen counter endseq\n",
      "000000186308\n",
      "Actual:    startseq jumbo jet lifting off the runway at takeoff endseq\n",
      "Predicted:  startseq british airways plane taking off at the airport endseq\n",
      "000000269945\n",
      "Actual:    startseq booth for or by firefighters at fair endseq\n",
      "Predicted:  startseq group of teddy bears that are sitting next to each other endseq\n",
      "000000569767\n",
      "Actual:    startseq two people are riding horses down trail endseq\n",
      "Predicted:  startseq herd of cattle grazing on top of grass covered field endseq\n",
      "000000458339\n",
      "Actual:    startseq toilet in bathroom filled with human waste endseq\n",
      "Predicted:  startseq bath room with tiled floor and toilet endseq\n",
      "000000136433\n",
      "Actual:    startseq person who is walking with some luggage behind them endseq\n",
      "Predicted:  startseq black and white photograph of man playing guitar endseq\n",
      "000000283222\n",
      "Actual:    startseq above view of someone with skis on their feet and snow endseq\n",
      "Predicted:  startseq skateboarder is in mid air during trick endseq\n",
      "000000302945\n",
      "Actual:    startseq black and white sign named north arizona near another smaller sign endseq\n",
      "Predicted:  startseq couple of street signs that read types of information endseq\n",
      "000000492719\n",
      "Actual:    startseq bathroom with white sink toilet and bathtub and marbled countertop endseq\n",
      "Predicted:  startseq bath room with white toilet and sink endseq\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.13650598977149334"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model1(model, train_descriptions,\"coco/full/features.h5\", word_to_index, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000000529966\n",
      "Actual:    startseq two bowls of oranges are sitting on metal surface endseq\n",
      "Predicted:  startseq variety of fruits and vegetables on table endseq\n",
      "000000125062\n",
      "Actual:    startseq line of small teddy bears are in front of several dvd cases endseq\n",
      "Predicted:  startseq group of teddy bears sitting next to each other endseq\n",
      "000000559160\n",
      "Actual:    startseq boy in sports jersey holds skateboard in right hand endseq\n",
      "Predicted:  startseq there is man riding skate board endseq\n",
      "000000266409\n",
      "Actual:    startseq man that has ski and is standing in the snow endseq\n",
      "Predicted:  startseq group of cross country paused on cross country path endseq\n",
      "000000154705\n",
      "Actual:    startseq computer sitting on top of white table with another computer endseq\n",
      "Predicted:  startseq living room filled with lots of different computer screens endseq\n",
      "000000031620\n",
      "Actual:    startseq woman in white dress and man in gray stand near cake on white table under white canopy endseq\n",
      "Predicted:  startseq group of people who are sitting around table endseq\n",
      "000000250901\n",
      "Actual:    startseq three people opening wrapped sandwich out on the grass endseq\n",
      "Predicted:  startseq group of people who are eating some kind of pizza endseq\n",
      "000000528862\n",
      "Actual:    startseq group of giraffes wonder around in fenced off area with other animals endseq\n",
      "Predicted:  startseq group of giraffes walking in front of trees endseq\n",
      "000000526751\n",
      "Actual:    startseq boat stowed up on the beaches on sand endseq\n",
      "Predicted:  startseq an image of boat in the water next to waterway endseq\n",
      "000000109055\n",
      "Actual:    startseq cat sitting next to bunch of bikes parked next to each other endseq\n",
      "Predicted:  startseq bunch of orange cones parked next to each other endseq\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.09066516969251226"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model1(model, val_descriptions,\"coco/full/val_features.h5\", word_to_index, max_length,dataset=\"val2017\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
